# Performance Report: Finance LLM (Qwen3 8B)

**Date:** November 2, 2025  
**Model:** DragonLLM/qwen3-8b-fin-v1.0  
**Backend:** Transformers (PyTorch)  
**Hardware:** L4x1 GPU (24GB VRAM)

---

## Executive Summary

âœ… **System is operational** with good performance for single-user scenarios  
âš ï¸ **Parallelization is limited** - concurrent requests queue up  
ðŸ’¡ **Optimization recommended** for production multi-user deployment

---

## Performance Metrics

### Inference Speed
- **Average:** ~14.9 tokens/second
- **Single request (50 tokens):** 13.9 tokens/s
- **Response time:** 
  - Short answers (50 tokens): ~3.6s
  - Medium answers (150 tokens): ~10-12s
  - Long answers (200 tokens): ~13-15s

### Quality Metrics
- **English tests:** 8/8 passed (100%)
- **French tests:** 10/10 passed (100%)
- **Token efficiency:** 100% (model uses full max_tokens allocation)
- **Answer completeness:** 100% (all answers complete with reasoning)

### Concurrent Request Handling
| Concurrent Requests | Total Time | Speedup | Throughput |
|---------------------|------------|---------|------------|
| 1 (baseline)        | 3.59s      | 1.0x    | 13.9 tok/s |
| 2 parallel          | 6.79s      | 1.52x   | 14.7 tok/s |
| 3 parallel          | 10.01s     | 2.34x   | 15.0 tok/s |

**Finding:** System shows some parallelization, but requests still queue. Uvicorn handles concurrency at the HTTP level, but model inference is sequential.

---

## Current Hardware: L4x1

**Specifications:**
- GPU: NVIDIA L4
- VRAM: 24 GB
- vCPU: 15 cores
- RAM: 44 GB
- Cost: **$0.70/hour** ($521/month)

**Performance:**
- âœ… Excellent for single-user, sequential requests
- âœ… Handles model (8B params) comfortably
- âš ï¸ Limited parallelization due to single GPU
- âš ï¸ Requests queue when multiple users access simultaneously

---

## GPU Load Analysis

### Current Bottlenecks

1. **Sequential Inference:**
   - Transformers library processes one request at a time
   - No native batching support in current implementation
   - GPU utilization drops between requests

2. **Memory Constraints:**
   - Model occupies ~16-18 GB VRAM (FP16/BF16)
   - Limited headroom for batch processing
   - KV cache grows with context length

3. **Throughput Ceiling:**
   - Maximum sustainable throughput: ~15 tokens/s
   - With 3 concurrent users: ~5 tokens/s per user
   - Queue latency increases with load

### Does GPU Load Slow Down Inference?

**YES, in these scenarios:**
- âœ… Multiple concurrent requests â†’ queuing delays
- âœ… Long context (>2K tokens) â†’ memory pressure
- âœ… High request rate (>10/min) â†’ sustained high load

**NO, for single requests:**
- Model runs at full speed (~15 tok/s)
- GPU is not thermally throttled
- Performance is consistent

---

## Upgrade Analysis: L40s

### Hardware Comparison

| Specification | L4x1 | L40s | Improvement |
|---------------|------|------|-------------|
| VRAM          | 24 GB | 48 GB | 2x |
| Compute (TFLOPS) | 242 | 362 | 1.5x |
| vCPU          | 15 | 30 | 2x |
| RAM           | 44 GB | 92 GB | 2x |
| **Cost/month** | **$521** | **$1,153** | **+$632 (+121%)** |

### Expected Benefits

**Inference Speed:**
- âœ… **1.5-2x faster** per request (~20-25 tokens/s)
- âœ… Lower latency for individual requests
- âœ… Faster model loading and warmup

**Parallelization:**
- âœ… **2-3x more concurrent requests** (6-9 simultaneous)
- âœ… Larger batch sizes possible
- âœ… Better GPU utilization
- âœ… Support for continuous batching

**Capacity:**
- âœ… Handle **20-30 requests/minute** sustainably
- âœ… Support **5-10 concurrent users** with <5s latency
- âœ… Headroom for peak traffic

### When to Upgrade to L40s

**RECOMMENDED if:**
- âœ… Expecting >20 requests/minute
- âœ… Multiple concurrent users (5+)
- âœ… Latency requirements <5 seconds
- âœ… Production deployment with SLA
- âœ… Budget allows +$632/month

**NOT NEEDED if:**
- âœ… Development/testing environment
- âœ… Single user or sequential requests
- âœ… Low traffic (<10 requests/min)
- âœ… Cost is primary concern

---

## Optimization Recommendations

### 1. Software Optimizations (No Additional Cost)

**A. Implement Request Batching**
```python
# Pseudo-code for batching
class RequestBatcher:
    def __init__(self, max_batch_size=4, max_wait_ms=50):
        self.queue = []
        self.max_batch = max_batch_size
        self.max_wait = max_wait_ms
    
    async def add_request(self, request):
        self.queue.append(request)
        if len(self.queue) >= self.max_batch:
            return await self.process_batch()
        # Wait for more requests or timeout
```

**Benefits:**
- 2-3x throughput improvement
- Better GPU utilization
- Lower per-request cost

**B. Enable Flash Attention**
```python
# In transformers_provider.py
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation="flash_attention_2",  # Add this
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
```

**Benefits:**
- 1.5-2x faster attention computation
- Lower memory usage
- Longer context support

**C. Optimize Token Generation**
```python
# Use sampling instead of greedy for faster generation
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    top_k=50,  # Add top-k sampling
    num_beams=1,  # Disable beam search
)
```

### 2. Backend Switch: Transformers â†’ vLLM

**Benefits:**
- âœ… **Automatic batching** (continuous batching)
- âœ… **PagedAttention** for memory efficiency
- âœ… **3-5x throughput** improvement
- âœ… Built-in parallelization

**Trade-offs:**
- âš ï¸ Need to revert code changes (we just migrated away from vLLM!)
- âš ï¸ vLLM 0.11+ should support Qwen3 now
- âš ï¸ More complex deployment

**Recommendation:** Wait for vLLM 0.12+ with stable Qwen3 support

### 3. Caching Strategy

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=100)
def get_cached_response(question_hash):
    # Cache common questions
    pass
```

**Benefits:**
- Instant responses for repeated questions
- Reduced GPU load
- Lower costs

---

## Cost-Benefit Analysis

### Current Setup (L4x1)
- **Cost:** $521/month
- **Capacity:** 5-10 requests/min
- **Latency:** ~12s per request
- **Best for:** Development, low traffic

### With Software Optimizations (L4x1 + Batching)
- **Cost:** $521/month (no change)
- **Capacity:** 15-20 requests/min
- **Latency:** ~8-10s per request
- **Best for:** Production, medium traffic
- **ROI:** âœ…âœ…âœ… **HIGHEST** - Free performance gain

### Upgrade to L40s
- **Cost:** $1,153/month (+$632)
- **Capacity:** 30-50 requests/min
- **Latency:** ~5-7s per request  
- **Best for:** High traffic, strict SLA
- **ROI:** âœ… Good if traffic justifies

### Upgrade to L40s + Software Optimizations
- **Cost:** $1,153/month (+$632)
- **Capacity:** 50-100 requests/min
- **Latency:** ~3-5s per request
- **Best for:** Production at scale
- **ROI:** âœ…âœ… Excellent for >50 req/min

---

## Action Plan

### Phase 1: Immediate (No Cost)
1. âœ… **Implement request batching** - 2-3x throughput
2. âœ… **Enable Flash Attention** - 1.5x faster
3. âœ… **Add response caching** - Reduce load
4. âœ… **Monitor metrics** - Track improvements

**Expected Result:** 
- Throughput: 15 â†’ 30-40 requests/min
- Latency: 12s â†’ 8-10s
- Cost: No change

### Phase 2: If Needed (After 1-2 weeks)
1. Monitor traffic patterns
2. Measure actual vs expected load
3. If sustained >30 req/min â†’ Consider L40s upgrade
4. If <30 req/min â†’ Stay on L4x1

### Phase 3: Future Optimization
1. Evaluate vLLM 0.12+ when Qwen3 support is stable
2. Consider model quantization (INT8) for 2x speedup
3. Implement load balancing if traffic exceeds single GPU

---

## Conclusion

**Current State:**
- âœ… System works well for single-user scenarios
- âœ… Good inference speed (~15 tok/s)
- âš ï¸ Limited parallelization

**Recommendations:**
1. **Start with software optimizations** (batching, Flash Attention)
2. **Monitor traffic** for 1-2 weeks
3. **Upgrade to L40s** only if traffic justifies (+$632/month)
4. **Consider vLLM** when Qwen3 support improves

**Best ROI:** Software optimizations on L4x1 = Free 2-3x performance boost! ðŸš€

---

## Appendix: Test Results Summary

### English Finance Tests (8 tests)
- âœ… 100% success rate
- â±ï¸ Avg: 11.74s per response
- ðŸ“ Avg: 175 tokens
- ðŸš€ Speed: 14.91 tok/s

### French Finance Tests (10 tests)
- âœ… 100% success rate  
- â±ï¸ Avg: 12.03s per response
- ðŸ“ Avg: 180 tokens
- ðŸš€ Speed: 14.96 tok/s
- ðŸ‡«ðŸ‡· Excellent French terminology support

### Concurrent Performance
- 2 parallel: 1.52x speedup
- 3 parallel: 2.34x speedup
- Max observed: ~15 tok/s throughput

