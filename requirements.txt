# Minimal dependencies for vLLM deployment
# Note: vLLM deployments (HF Spaces and Koyeb) use Dockerfiles which install only langfuse and logfire
# vLLM provides its own OpenAI-compatible API server, so FastAPI/uvicorn/transformers are not needed
# This file is primarily for local development/testing

# Observability (used in both vLLM deployments and local dev)
langfuse>=2.50.0
logfire>=0.0.1
