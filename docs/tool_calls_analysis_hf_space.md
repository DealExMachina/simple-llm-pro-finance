# Analyse : Pourquoi les Tool Calls ne Fonctionnent Pas

## ğŸ” ProblÃ¨me IdentifiÃ©

L'API Hugging Face Space **ne supporte PAS les tool calls** dans son implÃ©mentation actuelle.

## ğŸ“‹ Analyse du Code

### 1. ModÃ¨le de RequÃªte (`app/models/openai.py`)

```python
class ChatCompletionRequest(BaseModel):
    model: Optional[str] = None
    messages: List[Message]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False
    top_p: Optional[float] = 1.0
    # âŒ PAS de champ "tools"
    # âŒ PAS de champ "tool_choice"
```

**ProblÃ¨me :** Le modÃ¨le Pydantic ne dÃ©finit pas les champs `tools` et `tool_choice`, donc mÃªme si PydanticAI les envoie, ils sont **ignorÃ©s** par FastAPI.

### 2. ModÃ¨le de RÃ©ponse (`app/models/openai.py`)

```python
class ChoiceMessage(BaseModel):
    role: Literal["assistant"]
    content: Optional[str] = None
    # âŒ PAS de champ "tool_calls"
```

**ProblÃ¨me :** Le modÃ¨le de rÃ©ponse ne dÃ©finit pas le champ `tool_calls`, donc mÃªme si le modÃ¨le gÃ©nÃ©rait des tool calls, ils ne seraient **pas retournÃ©s** dans la rÃ©ponse.

### 3. Provider Transformers (`app/providers/transformers_provider.py`)

```python
async def chat(self, payload: Dict[str, Any], stream: bool = False):
    messages = payload.get("messages", [])
    temperature = payload.get("temperature", DEFAULT_TEMPERATURE)
    max_tokens = payload.get("max_tokens", DEFAULT_MAX_TOKENS)
    top_p = payload.get("top_p", DEFAULT_TOP_P)
    # âŒ PAS d'extraction de "tools"
    # âŒ PAS d'extraction de "tool_choice"
    
    # GÃ©nÃ¨re juste du texte
    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    return {
        "choices": [{
            "message": {"role": "assistant", "content": generated_text},
            # âŒ PAS de "tool_calls"
        }]
    }
```

**ProblÃ¨me :** Le provider :
1. N'extrait pas `tools` du payload
2. Ne passe pas les tools au modÃ¨le
3. Ne parse pas les tool calls de la rÃ©ponse
4. Ne retourne pas de `tool_calls` dans la rÃ©ponse

## ğŸ”„ Flux Actuel

```
PydanticAI Agent
    â†“ (envoie tools dans la requÃªte)
FastAPI Router
    â†“ (parse avec ChatCompletionRequest - IGNORE tools)
TransformersProvider
    â†“ (n'extrait pas tools du payload)
Qwen 8B Model
    â†“ (gÃ©nÃ¨re du texte, pas de tool calls)
TransformersProvider
    â†“ (retourne juste content, pas tool_calls)
FastAPI Router
    â†“ (retourne ChoiceMessage sans tool_calls)
PydanticAI Agent
    â†“ (reÃ§oit tool_calls = [])
```

## âœ… Solution : Ajouter le Support des Tool Calls

### Ã‰tape 1 : Mettre Ã  Jour le ModÃ¨le de RequÃªte

```python
# app/models/openai.py

from typing import List, Literal, Optional, Dict, Any
from pydantic import BaseModel, Field

class Function(BaseModel):
    name: str
    description: Optional[str] = None
    parameters: Dict[str, Any]

class Tool(BaseModel):
    type: Literal["function"] = "function"
    function: Function

class ChatCompletionRequest(BaseModel):
    model: Optional[str] = None
    messages: List[Message]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False
    top_p: Optional[float] = 1.0
    tools: Optional[List[Tool]] = None  # âœ… AJOUTER
    tool_choice: Optional[Union[Literal["none", "auto"], Dict[str, Any]]] = None  # âœ… AJOUTER
```

### Ã‰tape 2 : Mettre Ã  Jour le ModÃ¨le de RÃ©ponse

```python
# app/models/openai.py

class FunctionCall(BaseModel):
    name: str
    arguments: str  # JSON string

class ToolCall(BaseModel):
    id: str
    type: Literal["function"] = "function"
    function: FunctionCall

class ChoiceMessage(BaseModel):
    role: Literal["assistant"]
    content: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None  # âœ… AJOUTER
```

### Ã‰tape 3 : Mettre Ã  Jour le Provider

Le provider doit :

1. **Extraire les tools du payload**
2. **Inclure les tools dans le prompt** (format spÃ©cial pour Qwen)
3. **Parser la rÃ©ponse** pour dÃ©tecter les tool calls
4. **Retourner les tool calls** dans la rÃ©ponse

**Option A : Format Textuel (Plus Simple)**

Si le modÃ¨le gÃ©nÃ¨re des tool calls en texte, parser la rÃ©ponse :

```python
def _parse_tool_calls(self, generated_text: str, tools: List[Tool]) -> List[ToolCall]:
    """Parse tool calls from generated text."""
    # Chercher des patterns comme:
    # <tool_call>
    # {"name": "calculer_valeur_future", "arguments": "{\"capital_initial\": 10000}"}
    # </tool_call>
    import re
    import json
    
    tool_calls = []
    pattern = r'<tool_call>\s*({.*?})\s*</tool_call>'
    matches = re.findall(pattern, generated_text, re.DOTALL)
    
    for i, match in enumerate(matches):
        try:
            call_data = json.loads(match)
            tool_calls.append(ToolCall(
                id=f"call_{i}",
                type="function",
                function=FunctionCall(
                    name=call_data["name"],
                    arguments=json.dumps(call_data.get("arguments", {}))
                )
            ))
        except Exception as e:
            logger.warning(f"Failed to parse tool call: {e}")
    
    return tool_calls
```

**Option B : Format JSON Structured Output**

Si le modÃ¨le supporte le JSON mode, forcer un format structurÃ© :

```python
# Dans le prompt, ajouter:
# "You must respond in JSON format with tool_calls array"
# Puis parser le JSON
```

### Ã‰tape 4 : Mettre Ã  Jour le Router

Le router doit passer les tools au provider :

```python
# app/routers/openai_api.py

payload: Dict[str, Any] = {
    "model": body.model or settings.model,
    "messages": [m.model_dump() for m in body.messages],
    "temperature": body.temperature or 0.7,
    "top_p": body.top_p or 1.0,
    "stream": body.stream or False,
}

# âœ… AJOUTER
if body.tools:
    payload["tools"] = [t.model_dump() for t in body.tools]
if body.tool_choice:
    payload["tool_choice"] = body.tool_choice
```

## ğŸ¯ StratÃ©gie de Mise en Å’uvre

### Phase 1 : Support Basique (Textuel)

1. âœ… Ajouter `tools` et `tool_choice` au modÃ¨le de requÃªte
2. âœ… Ajouter `tool_calls` au modÃ¨le de rÃ©ponse
3. âœ… Parser les tool calls depuis le texte gÃ©nÃ©rÃ©
4. âœ… Retourner les tool calls dans la rÃ©ponse

### Phase 2 : Support AvancÃ© (Structured Output)

1. ğŸ”„ Forcer le modÃ¨le Ã  gÃ©nÃ©rer du JSON structurÃ©
2. ğŸ”„ Parser le JSON pour extraire les tool calls
3. ğŸ”„ Valider les tool calls contre les tools fournis

### Phase 3 : Support Complet (Native)

1. ğŸ¯ Fine-tuner le modÃ¨le pour gÃ©nÃ©rer des tool calls natifs
2. ğŸ¯ Utiliser un format de sortie spÃ©cialisÃ©
3. ğŸ¯ Support complet du format OpenAI

## ğŸ“ Notes Importantes

### Limitations du ModÃ¨le Qwen 8B

Le modÃ¨le Qwen 8B fine-tunÃ© peut :
- âœ… GÃ©nÃ©rer du texte qui mentionne les outils
- âŒ Ne pas gÃ©nÃ©rer de tool calls au format OpenAI natif
- âŒ Ne pas structurer la rÃ©ponse avec `tool_calls`

### Solutions de Contournement

1. **Parser le texte** : Extraire les tool calls depuis le texte gÃ©nÃ©rÃ©
2. **Format spÃ©cialisÃ©** : Utiliser un format de prompt spÃ©cial pour forcer les tool calls
3. **Post-processing** : Analyser la rÃ©ponse et exÃ©cuter les outils mentionnÃ©s

## ğŸ”— Fichiers Ã  Modifier

1. `app/models/openai.py` : Ajouter `tools`, `tool_choice`, `tool_calls`
2. `app/providers/transformers_provider.py` : GÃ©rer les tools et parser les tool calls
3. `app/routers/openai_api.py` : Passer les tools au provider
4. Tests : Ajouter des tests pour les tool calls

## ğŸ“š RÃ©fÃ©rences

- [OpenAI Tool Calls Format](https://platform.openai.com/docs/guides/function-calling)
- [PydanticAI Tools Documentation](https://ai.pydantic.dev/tools/)
- [Qwen Model Documentation](https://huggingface.co/Qwen)

