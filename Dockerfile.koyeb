# Koyeb-optimized Dockerfile using official vLLM OpenAI image
# Based on Koyeb's proven vLLM deployment approach

FROM vllm/vllm-openai:latest

# Environment variables
ENV HF_HOME=/tmp/huggingface \
    VLLM_ATTENTION_BACKEND=FLASH_ATTN

# Create cache directories with proper permissions
USER root
RUN mkdir -p /tmp/huggingface && chmod 777 /tmp/huggingface

# Switch back to default user
USER 1000

# Expose vLLM default port
EXPOSE 8000

# Default model and settings - can be overridden via Koyeb env/args
ENV MODEL="DragonLLM/Qwen-Open-Finance-R-8B"
ENV MAX_MODEL_LEN="8192"
ENV DTYPE="bfloat16"

# Use vLLM's native OpenAI server entrypoint
# Model is specified via environment or command args
CMD ["--model", "DragonLLM/Qwen-Open-Finance-R-8B", "--trust-remote-code", "--dtype", "bfloat16", "--max-model-len", "8192", "--gpu-memory-utilization", "0.90"]
