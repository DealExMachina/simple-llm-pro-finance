# Koyeb-optimized Dockerfile using vLLM's native OpenAI API server
# This leverages vLLM's built-in optimizations: continuous batching, PagedAttention, CUDA graphs

FROM nvidia/cuda:12.4.0-devel-ubuntu22.04

# Build argument for cache control
ARG CACHE_BUST=20250125_vllm

ENV PYTHONUNBUFFERED=1 \
    DEBIAN_FRONTEND=noninteractive \
    HF_HOME=/tmp/huggingface \
    VLLM_ATTENTION_BACKEND=FLASH_ATTN \
    CUDA_VISIBLE_DEVICES=0

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        python3.11 \
        python3.11-dev \
        python3-pip \
        git \
        curl && \
    rm -rf /var/lib/apt/lists/* && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 && \
    python3 -m pip install --upgrade pip

WORKDIR /app

# Install PyTorch with CUDA 12.4
RUN pip install --no-cache-dir \
    torch>=2.5.0 \
    --index-url https://download.pytorch.org/whl/cu124

# Install vLLM with all CUDA optimizations
# vLLM includes: Flash Attention, PagedAttention, continuous batching, CUDA graphs
RUN pip install --no-cache-dir \
    vllm>=0.6.0 \
    huggingface-hub>=0.20.0

# Create non-root user and cache directories
RUN useradd -m -u 1000 user && \
    mkdir -p /tmp/huggingface /tmp/vllm && \
    chown -R user:user /app /tmp/huggingface /tmp/vllm

# Copy startup script
COPY start-vllm.sh /app/start-vllm.sh
RUN chmod +x /app/start-vllm.sh && chown user:user /app/start-vllm.sh

USER user

# vLLM OpenAI server default port
EXPOSE 8000

# Use vLLM's native OpenAI-compatible server
CMD ["/app/start-vllm.sh"]

